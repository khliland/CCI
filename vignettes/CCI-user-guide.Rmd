---
title: "CCI-user-guide"
author: "Christian Thorjussen"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{CCI-user-guide}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
# Introduction
This vignette demonstrates how to use the 'CCI' package to perform computational conditional independence testing. Conditional independence is a concept within statistics which states that two random variables are independent given that we know the value of a third variable. And it is a fundamental assumption in many statistical and scientific models, for instance in causal models, it is therefor very useful to be able to test these assumptions. Mathematically we can express conditional independence between $Y$ and $X$ given $Z$ as the null hypothesis: $p(Y|X,Z) = p(Y|Z)$, which means that the distribution of $Y$ given $X$ and $Z$ is the same as the distribution of $Y$ given only $Z$. A common notation for this is $Y \perp X | Z$, which reads as "Y is independent (perpendicular) of X given Z". 

The big caveat when it comes to conditional independence testing is that it is an ''impossible'' problem. Because the null hypothesis $Y \perp X | Z$ is a composite null hypothesis; the null hypothesis includes all joint distributions where $Y$ and $X$ are independent given $Z$, which is in theory, an infinite set. It is impossible to make a statistical test which covers an infinite set of distributions.   

However, we know that machine learning methods are excellent tools to approximate point estimates from a wide variety of distributions, furthermore, experience tells us that especially tree based models provides robust out-of-sample predictions. Combining these two ''heuristics'' facilitates a way to estimate a distribution of prediction performance under the null hypothesis of conditional independence. 

Here's how the CCI test works when testing the statement \( X \perp Y \mid Z \):

1. **Permuting \(Y\)**: Start by permuting \(Y\) into \(Y^p\), which breaks any existing dependency between \(X\) and \(Y\).
2. **Subset Selection and Model Estimation**: Take a subset of the data of size \(p\) (where \(0 < p < 1\)) and estimate the relationship \(X = f(Y^p, Z)\) using this subset.
3. **Performance Evaluation**: Use the remaining \(1-p\) portion of the data to evaluate the performance of the predictions from \(X = f(Y^p, Z)\) using a performance metric like RMSE.
4. **Generating the Null Distribution**: Repeat the above steps (e.g., 500 times) to generate a null distribution of the performance metric under the assumption of conditional independence.
5. **Test Statistic Calculation**: Finally, calculate the test statistic by estimating the relationship \(X = f(Y, Z)\) using the original (non-permuted) data.

The observed test statistic is compared against the null distribution. If it falls far from the bulk of the null distribution, it suggests a strong relationship between \(X\) and \(Y\) given \(Z\), violating the null hypothesis of conditional independence.


# Basic usage
The 'CCI' package can be installed from CRAN using the following command:
```{r setup}
library(CCI)
```
The 'CCI' package comes with a set of data generating functions, which can be used to generate data sets with known conditional independence relationships. Thorought this user guide, we will primarly use data generated from these functions, and we will provide short descriptions of the data generating functions as we go along.

# Example 1: Simple conditional independence test with Gaussian data
```{r setup}
data <- 

```

### Example 3 (Time series data)
CCI can also be used to test wheter two time series variables are conditionally independent. However, analyzing time series data with CCI requires a relatively large dataset due to the complexity and dependencies inherent in time series analysis. In this example we test if two (\(X\) and \(Y\) trends which diverge in a times series are conditional independent given previous lags of X. We then to the same for lags of Y to show that this test is rejected (using significance level = 0.05). 



```r
time_series <- function(n, phi1, phi2) {
  phi <- c(phi1, phi2)
  # We generate X
  X <- arima.sim(n = n, list(ar = phi))
  Y <- numeric(n)
  
  for (t in 3:n) {
    Y[t] <- 0.01 * t +  1.2 * X[t-1] +  0.7 * X[t-2] +  0.5 * X[t-2]*X[t-1] + rnorm(1, sd = 1)
  }
  data <- data.frame(Time = 1:n, X = X, Y = Y)
}
set.seed(1993) 
data <- time_series(n = 1500, phi1 = 0.9, phi2 = -0.5)

data$X_lag1 <- c(NA, data$X[-length(data$X)])  
data$X_lag2 <- c(NA, NA, data$X[-(length(data$X)-1):-(length(data$X))])

cor(data$Y, data$X) # Showing the correlation between Y and X, indicating that they are not independent

# Inputing Time as a conditioning variables for general trends.
data <- na.omit(data)
CCI.test(formula = X ~ Y | X_lag1 + Time, data = data, p = 0.5, nperm = 200, method = "xgboost")
CCI.test(formula = Y ~ X | X_lag1 + X_lag2 + Time, p = 0.5, nperm = 200, data = data, method = "xgboost")

data$Y_lag1 <- c(NA, data$Y[-length(data$Y)])  
data$Y_lag2 <- c(NA, NA, data$Y[-(length(data$Y)-1):-(length(data$Y))])
CCI.test(formula = Y ~ X | Y_lag1 + Y_lag2 + Time, p = 0.5, nperm = 200, data = data, method = "xgboost", parametric = T)
```

















