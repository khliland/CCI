% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/CCI.tuner.R
\name{CCI.pretuner}
\alias{CCI.pretuner}
\alias{tuner}
\title{CCI tuner function for CCI test}
\usage{
CCI.pretuner(
  formula,
  data,
  method = "rf",
  validation_method = "cv",
  folds = 5,
  training_share = 0.7,
  tune_length = 3,
  seed = 1984,
  metric = "RMSE",
  random_grid = TRUE,
  samples = 30,
  data_type = "continuous",
  poly = TRUE,
  degree = 3,
  interaction = TRUE,
  verboseIter = FALSE,
  trace = FALSE,
  include_explanatory = FALSE,
  verbose = FALSE,
  size = 1:5,
  decay = c(0.001, 0.01, 0.1, 0.2, 0.5, 1),
  mtry = 1:5,
  nrounds = seq(50, 200, by = 25),
  eta = seq(0.01, 0.3, by = 0.05),
  max_depth = 1:6,
  subsample = seq(0.5, 1, by = 0.1),
  gamma = seq(0, 5, by = 1),
  colsample_bytree = seq(0.5, 1, by = 0.1),
  min_child_weight = 1:5,
  sigma = seq(0.1, 2, by = 0.3),
  C = seq(0.1, 2, by = 0.5),
  ...
)
}
\arguments{
\item{formula}{Model formula specifying the relationship between dependent and independent variables.}

\item{data}{A data frame containing the variables specified in the formula.}

\item{method}{Character. Specifies the machine learning method to use. Supported methods are random forest "rf", extreme gradient boosting "xgboost", neural-net "nnet, Gaussian Process Regression "gpr" and Support Vector Machine "svm".}

\item{validation_method}{Character. Specifies the resampling method. Default is "cv".}

\item{folds}{Integer. The number of folds for cross-validation during the tuning process. Default is 10.}

\item{training_share}{Numeric. For leave-group out cross-validation: the training percentage. Default is 0.7.}

\item{tune_length}{Integer. The number of parameter combinations to try during the tuning process. Default is 10.}

\item{seed}{Integer. The seed for random number generation. Default is 1984.}

\item{metric}{Character. The performance metric to optimize during tuning. Defaults to 'RMSE' for continuous data. Automatically set to 'Accuracy' for binary or categorical data.}

\item{random_grid}{Logical. If TRUE, a random grid search is performed. If FALSE, a full grid search is performed. Default is TRUE.}

\item{samples}{Integer. The number of random samples to take from the grid. Default is 30.}

\item{data_type}{Character. Specifies the type of data of dependent variable: "continuous", "binary", or "categorical". Default is "continuous".}

\item{poly}{Logical. If TRUE, polynomial terms of the conditional variables are included in the model. Default is TRUE.}

\item{degree}{Integer. The degree of polynomial terms to include if poly is TRUE. Default is 3.}

\item{interaction}{Logical. If TRUE, interaction terms of the conditional variables are included in the model. Default is TRUE.}

\item{verboseIter}{Logical. If TRUE, the function will print the tuning process. Default is FALSE.}

\item{trace}{Logical. If TRUE, the function will print the tuning process. Default is FALSE.}

\item{include_explanatory}{Logical. If TRUE, given the condition Y \emph{||} X |  Z, the function will include explanatory variable X in the model for Y. Default is FALSE}

\item{size}{Integer. The size of the neural network. Default is 1:5.}

\item{decay}{Numeric. The decay parameter for the neural network. Default is c(0, 0.01, 0.1).}

\item{mtry}{Integer. The number of variables randomly sampled as candidates at each split for random forest. Default is 1:5.}

\item{nrounds}{Integer. The number of rounds (trees) for methods such as xgboost and random forest. Default is seq(50, 200, by = 25).}

\item{eta}{Numeric. The learning rate for xgboost. Default is seq(0.01, 0.3, by = 0.05).}

\item{max_depth}{Integer. The maximum depth of the tree for xgboost. Default is 1:6.}

\item{subsample}{Numeric. The subsample ratio of the training instances for xgboost. Default is seq(0.5, 1, by = 0.1).}

\item{gamma}{Numeric. The minimum loss reduction required to make a further partition on a leaf node for xgboost. Default is seq(0, 5, by = 1).}

\item{colsample_bytree}{Numeric. The subsample ratio of columns when constructing each tree for xgboost. Default is seq(0.5, 1, by = 0.1).}

\item{min_child_weight}{Integer. The minimum sum of instance weight (hessian) needed in a child for xgboost. Default is 1:5.}

\item{sigma}{Numeric. The standard deviation of the Gaussian kernel for Gaussian Process Regression. Default is seq(0.1, 2, by = 0.3).}

\item{C}{Numeric. The regularization parameter for Support Vector Machine. Default is seq(0.1, 2, by = 0.5).}

\item{...}{Additional arguments to pass to the \code{CCI.tuner} function.}
}
\value{
A list containing:
\itemize{
\item \code{best_param}: A data frame with the best parameters and their performance metric.
\item \code{tuning_result}: A data frame with all tested parameter combinations and their performance metrics.
\item \code{warnings}: A character vector of warnings issued during tuning.
}
}
\description{
The \code{CCI.tuner} function performs a grid search over parameters for a conditional independence test using machine learning model supported by CCI.test. The tuner use the caret package for tuning.
}
\examples{
set.seed(123)
data <- data.frame(x1 = rnorm(100), x2 = rnorm(100), y = rnorm(100))
# Tune random forest parameters
result <- CCI.pretuner(formula = y ~ x1 | x2, data = data, seed = 192, samples = 5, folds = 3, method = "rf")
# Returns a list with best parameters and tuning results
if (requireNamespace("xgboost", quietly = TRUE)) {
  # Tune xgboost parameters
  result_xgb <- CCI.pretuner(formula = y ~ x1 | x2, data = data, seed = 192, samples = 5, folds = 3, method = "xgboost")
}
}
\seealso{
\code{\link{CCI.test} \link{perm.test}}, \code{\link{print.summary.CCI}}, \code{\link{plot.CCI}}, \code{\link{QQplot}}
}
