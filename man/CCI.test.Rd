% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/CCI.test.R
\name{CCI.test}
\alias{CCI.test}
\alias{CCI}
\title{Computational test for conditional independence based on ML and Monte Carlo Cross Validation}
\usage{
CCI.test(
  formula = NULL,
  data,
  plot = TRUE,
  p = 0.5,
  nperm = 60,
  nrounds = 600,
  dag = NULL,
  dag_n = 1,
  metric = "Auto",
  method = "rf",
  choose_direction = FALSE,
  print_result = TRUE,
  parametric = FALSE,
  poly = TRUE,
  degree = 3,
  subsampling = 1,
  num_class = NULL,
  interaction = TRUE,
  metricfunc = NULL,
  mlfunc = NULL,
  tail = NA,
  tune = FALSE,
  samples = 35,
  folds = 5,
  tune_length = 10,
  seed = NA,
  random_grid = TRUE,
  nthread = 1,
  ...
)
}
\arguments{
\item{formula}{Model formula or a DAGitty object specifying the relationship between dependent and independent variables.}

\item{data}{A data frame containing the variables specified in the formula.}

\item{plot}{Logical, indicating if a plot of the null distribution with the test statistic should be generated. Default is TRUE.}

\item{p}{Numeric. Proportion of data used for training the model. Default is 0.5.}

\item{nperm}{Integer. The number of permutations to perform. Default is 600.}

\item{nrounds}{Integer. The number of rounds (trees) for methods such as xgboost and random forest. Default is 120.}

\item{dag}{An optional DAGitty object for specifying a Directed Acyclic Graph (DAG) to use for conditional independence testing. Default is NA.}

\item{dag_n}{Integer. If a DAGitty object is provided, specifies which conditional independence test to perform. Default is 1.}

\item{metric}{Character. Specifies the type of data: "Auto", "RMSE" or "Kappa". Default is "Auto".}

\item{method}{Character. Specifies the machine learning method to use. Supported methods include generlaized linear models "lm", random forest "rf", and extreme gradient boosting "xgboost", etc. Default is "rf".#'}

\item{choose_direction}{Logical. If TRUE, the function will choose the best direction for testing. Default is FALSE.}

\item{print_result}{Logical. If TRUE, the function will print the result of the test. Default is TRUE.}

\item{parametric}{Logical, indicating whether to compute a parametric p-value instead of the empirical p-value. A parametric p-value assumes that the null distribution is gaussian. Default is FALSE.}

\item{poly}{Logical. If TRUE, polynomial terms of the conditional variables are included in the model. Default is TRUE.}

\item{degree}{Integer. The degree of polynomial terms to include if poly is TRUE. Default is 3.}

\item{subsampling}{Numeric. The proportion of data to use for subsampling. Default is 1 (no subsampling).}

\item{num_class}{Integer. The number of classes for categorical data (used in xgboost). Default is NULL.}

\item{interaction}{Logical. If TRUE, interaction terms of the conditional variables are included in the model. Default is TRUE.}

\item{metricfunc}{Optional the user can pass a custom function for calculating a performance metric based on the model's predictions. Default is NULL.}

\item{mlfunc}{Optional the user can pass a custom machine learning wrapper function to use instead of the predefined methods. Default is NULL.}

\item{tail}{Character. Specifies whether to calculate left-tailed or right-tailed p-values, depending on the performance metric used. Only applicable if using \code{metricfunc} or \code{mlfunc}. Default is NA.}

\item{tune}{Logical. If TRUE, the function will perform hyperparameter tuning for the specified machine learning method. Default is FALSE.}

\item{samples}{Integer. The number of samples to use for tuning. Default is 35.}

\item{folds}{Integer. The number of folds for cross-validation during the tuning process. Default is 5.}

\item{tune_length}{Integer. The number of parameter combinations to try during the tuning process. Default is 10.}

\item{seed}{Integer. The seed for tuning. Default is NA.}

\item{random_grid}{Logical. If TRUE, a random grid search is performed. If FALSE, a full grid search is performed. Default is TRUE.}

\item{nthread}{Integer. The number of threads to use for parallel processing. Default is 1.}

\item{...}{Additional arguments to pass to the \code{perm.test} function.}
}
\value{
Invisibly returns the result of \code{perm.test}, which is an object of class 'CCI' containing the null distribution, observed test statistic, p-values, the machine learning model used, and the data.
}
\description{
The \code{CCI.test} function performs a conditional independence test using a specified machine learning model or a custom model provided by the user. It calculates the test statistic, generates a null distribution via permutations, computes p-values, and optionally generates a plot of the null distribution with the observed test statistic.
The 'CCI.test' function serves as a wrapper around the 'perm.test' function
}
\examples{
set.seed(123)

# Example: Basic use with a continuous outcome. The tests if y is independent of x1 given x2.

data <- data.frame(x1 = stats::rnorm(100), x2 = stats::rnorm(100), y = stats::rnorm(100))
result <- CCI.test(y ~ x1 | x2, data = data, nperm = 25, interaction = FALSE)
summary(result)
# Example: Using xgboost when y is categorical
data <- data.frame(x1 = stats::rnorm(100), x2 = stats::rnorm(100), x3 = stats::rnorm(100),
                   y = sample(1:3, 100, replace = TRUE) - 1)
result <- CCI.test(y ~ x1 | x2 + x3, data = data, method = "xgboost",
                   metric = "Kappa", nperm = 25, num_class = 3)
# Example: Again we can switch y and x1 (still using xgboost)
data <- data.frame(x1 = stats::rnorm(100), x2 = stats::rnorm(100), x3 = stats::rnorm(100),
                   y = sample(1:3, 100, replace = TRUE) - 1)
result <- CCI.test(x1 ~ y | x2 + x3, data = data, method = "xgboost", nperm = 25)
# Example testing with a linear model as:
ml_wrapper <- function(formula, data, train_indices, test_indices, ...) {
  model <- lm(formula, data = data[train_indices, ])
  predictions <- predict(model, newdata = data[test_indices, ])
  actual <- data[test_indices, ][[all.vars(formula)[1]]]
  metric <- sqrt(mean((abs(predictions - actual))))
  return(metric)
}

result <- CCI.test(y ~ x1 | x2, data = data, nperm = 200,
                   mlfunc = ml_wrapper, tail = "right")
summary(result)
# Example: Using a custom performance metric function
data_generator <-  function(N){
  Z1 <- rnorm(N,0,1)
  Z2 <- rnorm(N,0,1)
  X <- rnorm(N, Z1 + Z2, 1)
  Y <- rnorm(N, Z1 + Z2, 1)
  df <- data.frame(Z1, Z2, X, Y)
  return(df)
}

data <- data_generator(500)
Rsquare_metric  <- function(actual, predictions, ...) {
  sst <- sum((actual - mean(actual))^2)
  ssr <- sum((actual - predictions)^2)
  metric <- 1 - (ssr / sst)
  return(metric)
}
debug(test.gen)
correct_test <- CCI.test(Y ~ X | Z1 + Z2, data = data,
                         metricfunc = Rsquare_metric, tail = "right")
false_test <- CCI.test(Y ~ X | Z1, data = data,
                       metricfunc = Rsquare_metric, tail = "right")
}
\seealso{
\code{\link{perm.test}}, \code{\link{print.summary.CCI}}, \code{\link{plot.CCI}}, \code{\link{CCI.pretuner}}, \code{\link{QQplot}}
}
